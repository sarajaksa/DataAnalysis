{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Kaggle competition, the goal is to find out if the Quora questions are actually seeking answers, or they are simply retorical questions and searching for flame wars. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the competition is question: https://www.kaggle.com/c/quora-insincere-questions-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"#imports\">Imports</a>\n",
    "* <a href=\"#subset\">Creating Smaller Subsets for Analysis</a>\n",
    "* <a href=\"#load\">Loading Files</a>\n",
    "* <a href=\"#length\">Analysis based on Length</a>\n",
    "    * <a href=\"#letterlength\">Length based on Letter-Count</a>\n",
    "    * <a href=\"#wordnum\">Analysis based on Word-Count</a>\n",
    "    * <a href=\"#wordlength\">Analysis based on avreage length of the word</a>\n",
    "* <a href=\"#letter\">Analysis Based on Letters</a>\n",
    "    * <a href=\"#letter1\">Letters used</a>\n",
    "    * <a href=\"#lettergroups\">Groups of characters (letters - upper and lower, numbers, punctuation)</a>\n",
    "    * <a href=\"#letter2\">Letters used (group of 2)</a>\n",
    "* <a href=\"#words\">Analysis based on Words</a>\n",
    "    * <a href=\"#stopwords\">Number of stop words in a sentence</a>\n",
    "* <a href=\"#phonetics\">Phonetics</a>\n",
    "    * <a href=\"#typeofsounds\">Type of sounds</a>\n",
    "* <a href=\"#morphology\">Morphology</a>\n",
    "    * <a href=\"#morphologypinker\">Morphology - Pinker Example</a>\n",
    "* <a href=\"#pos\">Parts of Speech</a>\n",
    "* <a href=\"#diversity\">Lexical Diversity</a>\n",
    "* <a href=\"#bayes\">Bayes Analysis of all Features</a>\n",
    "* <a href=\"#logistic\">Logistic Regression of all Features</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"imports\">Imports</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phonetics chapter assumes that the espeak is also downloaded on the computer. It is only relevant for that part of the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "import lexical_diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also the command to download the nltk databases. I used punkt, stopwords, averaged_perceptron_tagger, universal_tagset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(download_dir=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"subset\">Creating Smaller Subset for Analysis</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because I have a four year old computer with a a smaller amount of RAM, I have decided to make all my analysis with the 2000 elements, with 1000 elements from each class. In order to make this file, I have created it with the prodecure described below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I opened the file with the original data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/train.csv\", \"r\") as read:\n",
    "    questions_data = pandas.read_csv(read, sep=\",\", header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I created the new pandas dataframe. I added 1000 elements for each class (900 for training and 100 for testing). I had added the random state, becase this acts like the seed and allows for the replication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame0 = questions_data.query('target == 0').sample(n=1000, random_state=2000)\n",
    "DataFrame1 = questions_data.query('target == 1').sample(n=1000, random_state=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDataFrameTrain = pandas.DataFrame()\n",
    "newDataFrameTrain = newDataFrameTrain.append(DataFrame0[100:])\n",
    "newDataFrameTrain = newDataFrameTrain.append(DataFrame1[100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDataFrameTest = pandas.DataFrame()\n",
    "newDataFrameTest = newDataFrameTest.append(DataFrame0[:100])\n",
    "newDataFrameTest = newDataFrameTest.append(DataFrame1[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I sorted the new dataframe based on index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDataFrameTrain.sort_index(inplace=True)\n",
    "newDataFrameTest.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And saved the whole thing in a file, so I don't need to repeat this every single time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDataFrameTrain.to_csv(\"data/sara_train.csv\", index=False)\n",
    "newDataFrameTest.to_csv(\"data/sara_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"load\">Loading Files</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am now loading up the subset of all the elements, that I am going to use in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/sara_train.csv\", \"r\") as read:\n",
    "    train = pandas.read_csv(read, sep=\",\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/sara_test.csv\", \"r\") as read:\n",
    "    test = pandas.read_csv(read, sep=\",\", header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"length\">Analysis based on Length</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am going to calculate different statistics based on length of the text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"letterlength\">Length based on Letter-Count</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first calculate the length of the question. And then let us check if there is any diffeence in the length of the text, based on the class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"length\"] = [len(question) for question in train[\"question_text\"]]\n",
    "test[\"length\"] = [len(question) for question in test[\"question_text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there any difference in the mean of the question length? It seems so. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66.91222222222223, 97.62333333333333)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.mean(train.query('target == 0')[\"length\"]), numpy.mean(train.query('target == 1')[\"length\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are differences in the mean of the length of the question, let us check their distribution and how much is it overlapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Differences in length')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = plt.figure(figsize=(20,8))\n",
    "plot1 = f.add_subplot(1, 1, 1)\n",
    "plot1.hist(train.query('target == 0')[\"length\"], alpha=.4)\n",
    "plot1.hist(train.query('target == 1')[\"length\"], alpha=.4)\n",
    "plot1.set_title(\"Differences in length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let us check some of the other values for this two distributions. First the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57.0, 88.5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.median(train.query('target == 0')[\"length\"]), numpy.median(train.query('target == 1')[\"length\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the maxsimum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(242, 250)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.max(train.query('target == 0')[\"length\"]), numpy.max(train.query('target == 1')[\"length\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the minimum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 17)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.min(train.query('target == 0')[\"length\"]), numpy.min(train.query('target == 1')[\"length\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us put these now in a more graphical representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Differences in length - class 1')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = plt.figure(figsize=(20,8))\n",
    "plot1 = f.add_subplot(1, 2, 1)\n",
    "plot1.boxplot(train.query('target == 0')[\"length\"])\n",
    "plot1.set_title(\"Differences in length - class 0\")\n",
    "plot2 = f.add_subplot(1, 2, 2)\n",
    "plot2.boxplot(train.query('target == 1')[\"length\"])\n",
    "plot2.set_title(\"Differences in length - class 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that, based on some descriptive statistics, the length of the text based on letter count can be somehow used to differentiate the samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check for how much, I am going to put this in the logistic regression. One thing to keep in mind is, that logistic regression (just like linear regression) does not do well with multicolliniarity. But this is only going to be point latter on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_len_letter = LogisticRegression(solver=\"lbfgs\")\n",
    "log_len_letter = log_len_letter.fit(numpy.array(train[\"length\"]).reshape(-1, 1), train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_len_letter_prediction = log_len_letter.predict(numpy.array(test[\"length\"]).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[69, 31],\n",
       "       [37, 63]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_len_letter_confusion_matrix = confusion_matrix(test[\"target\"], log_len_letter_prediction)\n",
    "log_len_letter_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(log_len_letter_confusion_matrix[0][0] + log_len_letter_confusion_matrix[1][1])/sum(sum(log_len_letter_confusion_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that with just the length of the question in characters, we can come up with about 66% accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"wordnum\">Analysis based on Word-Count</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us see, if I can get good results also with the word count. For this, let me first count the words. Here I simply devided the elements, the way that were seperated by space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"wordnum\"] = [len(question.split(\" \")) for question in train[\"question_text\"]]\n",
    "test[\"wordnum\"] = [len(question.split(\" \")) for question in test[\"question_text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see, if there is an avreage difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12.078888888888889, 17.25777777777778)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.mean(train.query('target == 0')[\"wordnum\"]), numpy.mean(train.query('target == 1')[\"wordnum\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I am also going to try the nltk way, just to see if there are differences. There are, but not by much. It just seems to find more words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13.573333333333334, 19.553333333333335)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"wordnum_nltk\"] = [len(word_tokenize(question)) for question in train[\"question_text\"]]\n",
    "test[\"wordnum_nltk\"] = [len(word_tokenize(question)) for question in test[\"question_text\"]]\n",
    "numpy.mean(train.query('target == 0')[\"wordnum_nltk\"]), numpy.mean(train.query('target == 1')[\"wordnum_nltk\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now also check in logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_len_letter = LogisticRegression(solver=\"lbfgs\")\n",
    "log_len_letter = log_len_letter.fit(numpy.array(train[\"wordnum\"]).reshape(-1, 1), train[\"target\"])\n",
    "log_len_letter_prediction = log_len_letter.predict(numpy.array(test[\"wordnum\"]).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[67, 33],\n",
       "       [38, 62]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_len_letter_confusion_matrix = confusion_matrix(test[\"target\"], log_len_letter_prediction)\n",
    "log_len_letter_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.645"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(log_len_letter_confusion_matrix[0][0] + log_len_letter_confusion_matrix[1][1])/sum(sum(log_len_letter_confusion_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.655"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_len_letter = LogisticRegression(solver=\"lbfgs\")\n",
    "log_len_letter = log_len_letter.fit(numpy.array(train[\"wordnum_nltk\"]).reshape(-1, 1), train[\"target\"])\n",
    "log_len_letter_prediction = log_len_letter.predict(numpy.array(test[\"wordnum_nltk\"]).reshape(-1, 1))\n",
    "log_len_letter_confusion_matrix = confusion_matrix(test[\"target\"], log_len_letter_prediction)\n",
    "log_len_letter_confusion_matrix\n",
    "(log_len_letter_confusion_matrix[0][0] + log_len_letter_confusion_matrix[1][1])/sum(sum(log_len_letter_confusion_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the number of words in the question gives us also 64% accuracy. The nltk is a bit higher, but not by much. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"wordlength\">Analysis based on avreage length of the word</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I am going to check if the length of the word in the question has any effect. The same as before, I will use for the word any string, that is seperated by space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"wordlength\"] = [numpy.mean([len(word) for word in question.split(\" \")]) for question in train[\"question_text\"]]\n",
    "test[\"wordlength\"] = [numpy.mean([len(word) for word in question.split(\" \")]) for question in test[\"question_text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see if there is a difference in the avreage length of the word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.720317502736821, 4.750773833840252)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.mean(train.query('target == 0')[\"wordlength\"]), numpy.mean(train.query('target == 1')[\"wordlength\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now for the nltk version as well. I will not also try to remove punctuation, since this would be words with length of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.700699640472918, 4.671388218595198)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"wordlength_nltk\"] = [numpy.mean([len(word) for word in word_tokenize(question) if len(word) > 1]) for question in train[\"question_text\"]]\n",
    "test[\"wordlength_nltk\"] = [numpy.mean([len(word) for word in word_tokenize(question) if len(word) > 1]) for question in test[\"question_text\"]]\n",
    "numpy.mean(train.query('target == 0')[\"wordlength_nltk\"]), numpy.mean(train.query('target == 1')[\"wordlength_nltk\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There does not seem to be any difference. Let us check this graphically as well, only for nltk version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Differences in avreage length of words - class 1')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = plt.figure(figsize=(20,8))\n",
    "plot1 = f.add_subplot(1, 2, 1)\n",
    "plot1.boxplot(train.query('target == 0')[\"wordlength_nltk\"])\n",
    "plot1.set_title(\"Differences in avreage length of words - class 0\")\n",
    "plot2 = f.add_subplot(1, 2, 2)\n",
    "plot2.boxplot(train.query('target == 1')[\"wordlength_nltk\"])\n",
    "plot2.set_title(\"Differences in avreage length of words - class 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there should be no predictive validity is this. But les us check anyway. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[60, 40],\n",
       "       [56, 44]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_len_letter = LogisticRegression(solver=\"lbfgs\")\n",
    "log_len_letter = log_len_letter.fit(numpy.array(train[\"wordlength\"]).reshape(-1, 1), train[\"target\"])\n",
    "log_len_letter_prediction = log_len_letter.predict(numpy.array(test[\"wordlength\"]).reshape(-1, 1))\n",
    "log_len_letter_confusion_matrix = confusion_matrix(test[\"target\"], log_len_letter_prediction)\n",
    "log_len_letter_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(log_len_letter_confusion_matrix[0][0] + log_len_letter_confusion_matrix[1][1])/sum(sum(log_len_letter_confusion_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.525"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_len_letter = LogisticRegression(solver=\"lbfgs\")\n",
    "log_len_letter = log_len_letter.fit(numpy.array(train[\"wordlength_nltk\"]).reshape(-1, 1), train[\"target\"])\n",
    "log_len_letter_prediction = log_len_letter.predict(numpy.array(test[\"wordlength_nltk\"]).reshape(-1, 1))\n",
    "log_len_letter_confusion_matrix = confusion_matrix(test[\"target\"], log_len_letter_prediction)\n",
    "log_len_letter_confusion_matrix\n",
    "(log_len_letter_confusion_matrix[0][0] + log_len_letter_confusion_matrix[1][1])/sum(sum(log_len_letter_confusion_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is barely any predictive vaildity, for either version. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"letter\">Analysis Based on Letters</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part of the analysis, I am going to try and see if there are any observed differences based on the letter and numbers used in the text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"letter1\">Letters used</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will check which symbols are even used in the texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_characters_in_texts = set()\n",
    "_ = [[all_characters_in_texts.add(letter) for letter in question] for question in train[\"question_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_characters_in_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'P)&Wf是M\\xa0慕G;…5羡kz4ōLm1éU[₹NnBd.妒H-DxsFIXcA样6和(%™河áq殤t0u8#S9E/,g”3] loVvyj>!’Z\"w7eJ2hT\\'。bçK*ü‘+a:不嫉C–$一pir的?O“YRQ\\u200b'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(list(all_characters_in_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am going to calculate for each question, now many times each character appear there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for letter in all_characters_in_texts:\n",
    "    train[letter] = [sum([1 for l in question if l == letter]) for question in train[\"question_text\"]]\n",
    "    test[letter] = [sum([1 for l in question if l == letter]) for question in test[\"question_text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now try how much can different letters help with prediction (I increased the number of iterations, since it would not converge with default 100). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_characters_in_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_len_letter = LogisticRegression(solver=\"lbfgs\", max_iter=10000)\n",
    "log_len_letter = log_len_letter.fit(numpy.array(train[list(all_characters_in_texts)]).reshape(-1, len(all_characters_in_texts)), train[\"target\"])\n",
    "log_len_letter_prediction = log_len_letter.predict(numpy.array(test[list(all_characters_in_texts)]).reshape(-1, len(all_characters_in_texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[68, 32],\n",
       "       [27, 73]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_len_letter_confusion_matrix = confusion_matrix(test[\"target\"], log_len_letter_prediction)\n",
    "log_len_letter_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.705"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(log_len_letter_confusion_matrix[0][0] + log_len_letter_confusion_matrix[1][1])/sum(sum(log_len_letter_confusion_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the letters, it is possible to get the accuracy to 71%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"lettergroups\">Groups of characters (letters - upper and lower, numbers, punctuation)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am also going to calculate the amount of some of the groups of characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = \"0123456789\"\n",
    "alpha_lower = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "alpha_upper = alpha_lower.upper()\n",
    "punctuation = \".\\\",;:!?'()&%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[n for n in numeric if n in all_characters_in_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[l for l in alpha_lower if l in all_characters_in_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[l for l in alpha_upper if l in all_characters_in_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', '\"', ',', ';', ':', '!', '?', \"'\", '(', ')', '&', '%']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p for p in punctuation if p in all_characters_in_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['是',\n",
       " '\\xa0',\n",
       " '慕',\n",
       " '…',\n",
       " '羡',\n",
       " 'ō',\n",
       " 'é',\n",
       " '[',\n",
       " '₹',\n",
       " '妒',\n",
       " '-',\n",
       " '样',\n",
       " '和',\n",
       " '™',\n",
       " '河',\n",
       " 'á',\n",
       " '殤',\n",
       " '#',\n",
       " '/',\n",
       " '”',\n",
       " ']',\n",
       " ' ',\n",
       " '>',\n",
       " '’',\n",
       " '。',\n",
       " 'ç',\n",
       " '*',\n",
       " 'ü',\n",
       " '‘',\n",
       " '+',\n",
       " '不',\n",
       " '嫉',\n",
       " '–',\n",
       " '$',\n",
       " '一',\n",
       " '的',\n",
       " '“',\n",
       " '\\u200b']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[o for o in all_characters_in_texts if not o in numeric + alpha_lower + alpha_upper + punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"numeric\"] = [sum([1 for l in question if l in numeric]) for question in train[\"question_text\"]]\n",
    "train[\"alphalower\"] = [sum([1 for l in question if l in alpha_lower]) for question in train[\"question_text\"]]\n",
    "train[\"alphaupper\"] = [sum([1 for l in question if l in alpha_upper]) for question in train[\"question_text\"]]\n",
    "train[\"punctuation\"] = [sum([1 for l in question if l in punctuation]) for question in train[\"question_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"numeric\"] = [sum([1 for l in question if l in numeric]) for question in test[\"question_text\"]]\n",
    "test[\"alphalower\"] = [sum([1 for l in question if l in alpha_lower]) for question in test[\"question_text\"]]\n",
    "test[\"alphaupper\"] = [sum([1 for l in question if l in alpha_upper]) for question in test[\"question_text\"]]\n",
    "test[\"punctuation\"] = [sum([1 for l in question if l in punctuation]) for question in test[\"question_text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am also going to calculate the ratio of all of these groups of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"numeric_ratio\"] = [sum([1 for l in question if l in numeric]) for question in train[\"question_text\"]]/train[\"length\"]\n",
    "train[\"alphalower_ratio\"] = [sum([1 for l in question if l in alpha_lower]) for question in train[\"question_text\"]]/train[\"length\"]\n",
    "train[\"alphaupper_ratio\"] = [sum([1 for l in question if l in alpha_upper]) for question in train[\"question_text\"]]/train[\"length\"]\n",
    "train[\"punctuation_ratio\"] = [sum([1 for l in question if l in punctuation]) for question in train[\"question_text\"]]/train[\"length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"numeric_ratio\"] = [sum([1 for l in question if l in numeric]) for question in test[\"question_text\"]]/test[\"length\"]\n",
    "test[\"alphalower_ratio\"] = [sum([1 for l in question if l in alpha_lower]) for question in test[\"question_text\"]]/test[\"length\"]\n",
    "test[\"alphaupper_ratio\"] = [sum([1 for l in question if l in alpha_upper]) for question in test[\"question_text\"]]/test[\"length\"]\n",
    "test[\"punctuation_ratio\"] = [sum([1 for l in question if l in punctuation]) for question in test[\"question_text\"]]/test[\"length\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now see, if adding these helps with the predictive validity. I will first try with the raw number of the groups of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_len_letter = LogisticRegression(solver=\"lbfgs\")\n",
    "log_len_letter = log_len_letter.fit(numpy.array(train[[\"numeric\",\"alphalower\",\"alphaupper\",\"punctuation\"]]).reshape(-1, 4), train[\"target\"])\n",
    "log_len_letter_prediction = log_len_letter.predict(numpy.array(test[[\"numeric\",\"alphalower\",\"alphaupper\",\"punctuation\"]]).reshape(-1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[65, 35],\n",
       "       [36, 64]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_len_letter_confusion_matrix = confusion_matrix(test[\"target\"], log_len_letter_prediction)\n",
    "log_len_letter_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.645"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(log_len_letter_confusion_matrix[0][0] + log_len_letter_confusion_matrix[1][1])/sum(sum(log_len_letter_confusion_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems data by using the raw number of group of characters, it is possible to get an accuracy of 64%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now try with ratios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_len_letter = LogisticRegression(solver=\"lbfgs\")\n",
    "log_len_letter = log_len_letter.fit(numpy.array(train[[\"numeric_ratio\",\"alphalower_ratio\",\"alphaupper_ratio\",\"punctuation_ratio\"]]).reshape(-1, 4), train[\"target\"])\n",
    "log_len_letter_prediction = log_len_letter.predict(numpy.array(test[[\"numeric_ratio\",\"alphalower_ratio\",\"alphaupper_ratio\",\"punctuation_ratio\"]]).reshape(-1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[44, 56],\n",
       "       [33, 67]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_len_letter_confusion_matrix = confusion_matrix(test[\"target\"], log_len_letter_prediction)\n",
    "log_len_letter_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.555"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(log_len_letter_confusion_matrix[0][0] + log_len_letter_confusion_matrix[1][1])/sum(sum(log_len_letter_confusion_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I guess the ratio is not as good predictor as the raw number. Possibly because of the length of the question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"letter2\">Letters used (group of 2)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I remeber correctly, sometimes (at least for words) they also check the groups of words. So 2-grams and 3-grams. Let me do the same thing with letters and see if it will predict anything. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_2_grams_letters = set()\n",
    "_ = [[all_2_grams_letters.add(\"\".join(n_gram).lower()) for n_gram in zip(question[:-1], question[1:])] for question in train[\"question_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'us',\n",
       " 'ob',\n",
       " 'oh',\n",
       " '是不',\n",
       " '(1',\n",
       " '!\"',\n",
       " 'sx',\n",
       " 't\\xa0',\n",
       " '3,',\n",
       " \"'e\",\n",
       " 'rp',\n",
       " 'xy',\n",
       " 'ii',\n",
       " 'ér',\n",
       " '-c',\n",
       " 'du',\n",
       " 'km',\n",
       " '32',\n",
       " '/8',\n",
       " 't/',\n",
       " 't-',\n",
       " 'gb',\n",
       " 'e?',\n",
       " 'lv',\n",
       " '(>',\n",
       " 'n)',\n",
       " 'nq',\n",
       " '/1',\n",
       " 'uj',\n",
       " \"'l\",\n",
       " 'iq',\n",
       " '0s',\n",
       " 'ke',\n",
       " \"l'\",\n",
       " 'zl',\n",
       " '/e',\n",
       " 'je',\n",
       " 'h1',\n",
       " 'o/',\n",
       " 'q ',\n",
       " 'yc',\n",
       " 'rg',\n",
       " '97',\n",
       " 'f?',\n",
       " 'j2',\n",
       " 'um',\n",
       " '/b',\n",
       " 'yf',\n",
       " 'og',\n",
       " 'n ',\n",
       " '22',\n",
       " '“s',\n",
       " 'dv',\n",
       " 'm)',\n",
       " 'o?',\n",
       " 'i6',\n",
       " 'dt',\n",
       " '**',\n",
       " '.t',\n",
       " 'v.',\n",
       " 'ng',\n",
       " 'vi',\n",
       " 'iz',\n",
       " 'mg',\n",
       " 'c)',\n",
       " '-6',\n",
       " 'n5',\n",
       " '(p',\n",
       " \"p'\",\n",
       " 'rb',\n",
       " '04',\n",
       " ' j',\n",
       " 'k\"',\n",
       " '+ ',\n",
       " '2 ',\n",
       " 'vs',\n",
       " 'p?',\n",
       " ', ',\n",
       " 'fd',\n",
       " '(g',\n",
       " 'é)',\n",
       " 'm,',\n",
       " '-m',\n",
       " 'sn',\n",
       " 'iv',\n",
       " 'uu',\n",
       " '25',\n",
       " 'ez',\n",
       " 'k/',\n",
       " 'iu',\n",
       " 'l)',\n",
       " 'd/',\n",
       " 'ka',\n",
       " 'p:',\n",
       " 'xo',\n",
       " 'sk',\n",
       " 'wf',\n",
       " '.5',\n",
       " ' o',\n",
       " '30',\n",
       " 'n’',\n",
       " 'lz',\n",
       " '19',\n",
       " '$5',\n",
       " ' c',\n",
       " '(a',\n",
       " '15',\n",
       " '7?',\n",
       " '13',\n",
       " '31',\n",
       " 'rk',\n",
       " '(f',\n",
       " 'nt',\n",
       " '. ',\n",
       " 'af',\n",
       " 'j ',\n",
       " 'hb',\n",
       " 'hd',\n",
       " '7-',\n",
       " ' (',\n",
       " 'gn',\n",
       " 'tb',\n",
       " ' t',\n",
       " '.)',\n",
       " 'nm',\n",
       " 'ot',\n",
       " 'jo',\n",
       " '(d',\n",
       " 'to',\n",
       " 'hh',\n",
       " 'pe',\n",
       " 'im',\n",
       " '.s',\n",
       " 'yz',\n",
       " 't!',\n",
       " '\"c',\n",
       " '80',\n",
       " '1?',\n",
       " 'tz',\n",
       " 'üm',\n",
       " '\\u200b,',\n",
       " '.l',\n",
       " 'ws',\n",
       " 't\"',\n",
       " 'r?',\n",
       " ' $',\n",
       " '95',\n",
       " 'n?',\n",
       " 'll',\n",
       " 'ux',\n",
       " '4x',\n",
       " '3.',\n",
       " 's;',\n",
       " '7/',\n",
       " 'i?',\n",
       " \"f'\",\n",
       " '!)',\n",
       " 'tp',\n",
       " 'ni',\n",
       " 'hl',\n",
       " 'es',\n",
       " 'bn',\n",
       " '78',\n",
       " '“i',\n",
       " '21',\n",
       " 'mp',\n",
       " 'u’',\n",
       " '29',\n",
       " 'hr',\n",
       " 'mr',\n",
       " 'a”',\n",
       " 'vt',\n",
       " 'dk',\n",
       " 'c.',\n",
       " '34',\n",
       " 'wt',\n",
       " 'sp',\n",
       " '0.',\n",
       " 'pc',\n",
       " 'li',\n",
       " '-2',\n",
       " 'ça',\n",
       " '.6',\n",
       " 'y’',\n",
       " \"w'\",\n",
       " 'rv',\n",
       " 'tx',\n",
       " 'ku',\n",
       " ')?',\n",
       " 'pd',\n",
       " 'sm',\n",
       " 'rs',\n",
       " 'kt',\n",
       " 'h4',\n",
       " ' r',\n",
       " 'h;',\n",
       " '0i',\n",
       " ' 4',\n",
       " 'vy',\n",
       " 'ip',\n",
       " 'n:',\n",
       " 'v ',\n",
       " 'py',\n",
       " 'ex',\n",
       " 'rd',\n",
       " '™ ',\n",
       " '\"h',\n",
       " 'd…',\n",
       " '\"l',\n",
       " '–b',\n",
       " '9:',\n",
       " ' 1',\n",
       " 'p\"',\n",
       " ' l',\n",
       " 'nw',\n",
       " '$4',\n",
       " '殤]',\n",
       " 'cb',\n",
       " 'u/',\n",
       " '\"b',\n",
       " 'wm',\n",
       " 'vo',\n",
       " '5t',\n",
       " '.a',\n",
       " 's\\u200b',\n",
       " ' ?',\n",
       " 'iw',\n",
       " 'e!',\n",
       " 'p;',\n",
       " 'oo',\n",
       " 'er',\n",
       " 'dz',\n",
       " 'ey',\n",
       " 's,',\n",
       " '“e',\n",
       " 'cu',\n",
       " '-i',\n",
       " 'vp',\n",
       " 'nz',\n",
       " 'dl',\n",
       " 'ye',\n",
       " 'u,',\n",
       " 'vl',\n",
       " ':1',\n",
       " 'ōt',\n",
       " 'nf',\n",
       " 'cp',\n",
       " 'rh',\n",
       " '6 ',\n",
       " 't–',\n",
       " 'ba',\n",
       " '(s',\n",
       " ' &',\n",
       " 'gg',\n",
       " \"'g\",\n",
       " 'mb',\n",
       " '96',\n",
       " 'bf',\n",
       " 'u ',\n",
       " 'ac',\n",
       " '.i',\n",
       " '+k',\n",
       " 'h.',\n",
       " 'dc',\n",
       " \"'a\",\n",
       " 'lb',\n",
       " 'db',\n",
       " 'jr',\n",
       " 'ia',\n",
       " '的。',\n",
       " 'dy',\n",
       " 'bs',\n",
       " 'k-',\n",
       " 'no',\n",
       " 'ou',\n",
       " \"'s\",\n",
       " 'ub',\n",
       " 'b3',\n",
       " '%?',\n",
       " '$8',\n",
       " 'gl',\n",
       " 'lh',\n",
       " ',\"',\n",
       " '.w',\n",
       " '/p',\n",
       " 't ',\n",
       " '91',\n",
       " 'ty',\n",
       " 'f\\xa0',\n",
       " 'h’',\n",
       " '\"g',\n",
       " 't.',\n",
       " '4?',\n",
       " '9.',\n",
       " 'ns',\n",
       " 'e)',\n",
       " 'b\"',\n",
       " 'zz',\n",
       " 'wr',\n",
       " 'a?',\n",
       " '’s',\n",
       " 'gv',\n",
       " '90',\n",
       " 'el',\n",
       " 'kn',\n",
       " 'ui',\n",
       " ' 2',\n",
       " 'te',\n",
       " 'jp',\n",
       " '/?',\n",
       " 'fa',\n",
       " 'jw',\n",
       " 'z?',\n",
       " 'ri',\n",
       " 'fe',\n",
       " 'by',\n",
       " 'xh',\n",
       " '12',\n",
       " 'i.',\n",
       " 'mm',\n",
       " 'f.',\n",
       " 'm/',\n",
       " 'gm',\n",
       " 'wo',\n",
       " '(e',\n",
       " 'rx',\n",
       " ' w',\n",
       " 'ff',\n",
       " 'ud',\n",
       " 'y\"',\n",
       " 'le',\n",
       " 'd-',\n",
       " 'yw',\n",
       " '2?',\n",
       " 'wd',\n",
       " 'rq',\n",
       " 'mf',\n",
       " 'nn',\n",
       " '不一',\n",
       " ' x',\n",
       " 'x-',\n",
       " '98',\n",
       " 'sc',\n",
       " '-h',\n",
       " '?’',\n",
       " 'dn',\n",
       " '2e',\n",
       " '4 ',\n",
       " '’t',\n",
       " 'ld',\n",
       " 'ir',\n",
       " 'gt',\n",
       " '28',\n",
       " 'd3',\n",
       " '\"m',\n",
       " 'tt',\n",
       " 'sj',\n",
       " 'c ',\n",
       " 's ',\n",
       " 'ls',\n",
       " ' f',\n",
       " '2.',\n",
       " '01',\n",
       " 'x ',\n",
       " 'ab',\n",
       " 'kz',\n",
       " 'e-',\n",
       " '5+',\n",
       " 'od',\n",
       " \"m'\",\n",
       " '\\xa0a',\n",
       " '-v',\n",
       " 'bv',\n",
       " '! ',\n",
       " 'ur',\n",
       " 'p,',\n",
       " ' 6',\n",
       " 'e/',\n",
       " 'js',\n",
       " 'b5',\n",
       " 'fk',\n",
       " 'p.',\n",
       " ' 9',\n",
       " '‘y',\n",
       " ' i',\n",
       " '40',\n",
       " 'lg',\n",
       " 'b-',\n",
       " 'yk',\n",
       " 'st',\n",
       " '00',\n",
       " '3)',\n",
       " 'os',\n",
       " 'an',\n",
       " '-g',\n",
       " 'lm',\n",
       " 'w,',\n",
       " ' h',\n",
       " 'yr',\n",
       " '\\xa0k',\n",
       " 'ta',\n",
       " '-y',\n",
       " 'ew',\n",
       " '(j',\n",
       " 'et',\n",
       " 'g,',\n",
       " 'yy',\n",
       " 'p3',\n",
       " '.?',\n",
       " '/m',\n",
       " 'op',\n",
       " 'o ',\n",
       " 'tu',\n",
       " ':8',\n",
       " 'gs',\n",
       " '3-',\n",
       " \"u'\",\n",
       " \"'v\",\n",
       " 'ne',\n",
       " ',0',\n",
       " 'lc',\n",
       " 'po',\n",
       " 'l”',\n",
       " '75',\n",
       " ' d',\n",
       " '8t',\n",
       " 'kv',\n",
       " 'ei',\n",
       " '-o',\n",
       " '河殤',\n",
       " 'cl',\n",
       " 'sd',\n",
       " 'az',\n",
       " ' b',\n",
       " ' n',\n",
       " 'ct',\n",
       " 'b?',\n",
       " 'fb',\n",
       " 'w.',\n",
       " 'en',\n",
       " '] ',\n",
       " 'dd',\n",
       " '\",',\n",
       " 'zy',\n",
       " 'e ',\n",
       " 'eh',\n",
       " '2,',\n",
       " '“y',\n",
       " \"g'\",\n",
       " 'va',\n",
       " 'gi',\n",
       " 'y/',\n",
       " 's(',\n",
       " 'hc',\n",
       " '2-',\n",
       " 'ze',\n",
       " '(k',\n",
       " 'di',\n",
       " 'me',\n",
       " '-p',\n",
       " 'ml',\n",
       " 'e,',\n",
       " 'd.',\n",
       " 'mu',\n",
       " 'pm',\n",
       " '9 ',\n",
       " 'eu',\n",
       " ' 7',\n",
       " 'gu',\n",
       " 'xa',\n",
       " 'g?',\n",
       " 's?',\n",
       " '-e',\n",
       " '\\xa0r',\n",
       " 'la',\n",
       " '35',\n",
       " 'ae',\n",
       " 'he',\n",
       " 'v?',\n",
       " 'aj',\n",
       " '-j',\n",
       " 'ro',\n",
       " '26',\n",
       " 'co',\n",
       " 'kh',\n",
       " ' q',\n",
       " 'ym',\n",
       " '20',\n",
       " 'k’',\n",
       " '.0',\n",
       " '/v',\n",
       " \"'d\",\n",
       " '4/',\n",
       " 'ic',\n",
       " '5 ',\n",
       " 'a’',\n",
       " 'ay',\n",
       " 'sl',\n",
       " 'k ',\n",
       " '-b',\n",
       " 'kf',\n",
       " '’ ',\n",
       " '-u',\n",
       " 'mc',\n",
       " 'g5',\n",
       " '\"u',\n",
       " 'hk',\n",
       " 'aw',\n",
       " 'pr',\n",
       " 'eg',\n",
       " '.1',\n",
       " 'h,',\n",
       " \"'m\",\n",
       " '慕和',\n",
       " 'e\\u200b',\n",
       " \"t'\",\n",
       " '/w',\n",
       " 'b,',\n",
       " 'w)',\n",
       " '54',\n",
       " '57',\n",
       " 'x?',\n",
       " 'e&',\n",
       " 'r’',\n",
       " '.o',\n",
       " 'kk',\n",
       " 't’',\n",
       " 'aq',\n",
       " 'xt',\n",
       " 'p ',\n",
       " '60',\n",
       " 'lu',\n",
       " '(i',\n",
       " \"s'\",\n",
       " 'au',\n",
       " '/i',\n",
       " '? ',\n",
       " 'ci',\n",
       " '一样',\n",
       " ' ₹',\n",
       " 'ej',\n",
       " '99',\n",
       " '.b',\n",
       " '“u',\n",
       " 'yg',\n",
       " 'pt',\n",
       " 'ja',\n",
       " '和嫉',\n",
       " 'o2',\n",
       " 'k.',\n",
       " 'ow',\n",
       " '-l',\n",
       " 'nd',\n",
       " 'ca',\n",
       " 'a:',\n",
       " 'xr',\n",
       " 'sf',\n",
       " 'y?',\n",
       " 'i)',\n",
       " 'it',\n",
       " 'as',\n",
       " '.\"',\n",
       " '.8',\n",
       " '\"6',\n",
       " 'so',\n",
       " ' [',\n",
       " 'rr',\n",
       " 'ee',\n",
       " '0 ',\n",
       " '79',\n",
       " '0t',\n",
       " 'my',\n",
       " '55',\n",
       " 'ha',\n",
       " 'w ',\n",
       " \" '\",\n",
       " '’m',\n",
       " 'l/',\n",
       " 'b ',\n",
       " 'ak',\n",
       " 'o-',\n",
       " 'xv',\n",
       " 's:',\n",
       " '(\"',\n",
       " 'k4',\n",
       " 'ut',\n",
       " 'pa',\n",
       " 'y,',\n",
       " '1,',\n",
       " 'ib',\n",
       " 'ma',\n",
       " 'iy',\n",
       " '&l',\n",
       " 'uz',\n",
       " '65',\n",
       " 'n,',\n",
       " 'br',\n",
       " 'gf',\n",
       " 'se',\n",
       " 'pu',\n",
       " 'd,',\n",
       " 'dh',\n",
       " '0%',\n",
       " '0c',\n",
       " '.c',\n",
       " ' p',\n",
       " ' .',\n",
       " 'ti',\n",
       " 'ju',\n",
       " 'ok',\n",
       " 'n”',\n",
       " 'ht',\n",
       " \"d'\",\n",
       " 'ds',\n",
       " '0?',\n",
       " '9/',\n",
       " 'r.',\n",
       " 'ie',\n",
       " 'u?',\n",
       " 'bj',\n",
       " ' g',\n",
       " 'sw',\n",
       " 'ly',\n",
       " ' 0',\n",
       " 'bu',\n",
       " '‘u',\n",
       " '\"-',\n",
       " 'ra',\n",
       " 'am',\n",
       " 'nc',\n",
       " ' v',\n",
       " 'ih',\n",
       " '/f',\n",
       " '- ',\n",
       " 'wl',\n",
       " '?\"',\n",
       " 'oj',\n",
       " 'c-',\n",
       " 'hs',\n",
       " '\"n',\n",
       " 'e.',\n",
       " 'sv',\n",
       " 'om',\n",
       " 'mz',\n",
       " '.e',\n",
       " 'k:',\n",
       " 't,',\n",
       " 'rw',\n",
       " 'ao',\n",
       " 'zg',\n",
       " 'bo',\n",
       " '& ',\n",
       " 'yo',\n",
       " 'yp',\n",
       " \"r'\",\n",
       " \"'n\",\n",
       " '2/',\n",
       " 'hu',\n",
       " 'nr',\n",
       " 'g)',\n",
       " 'y)',\n",
       " ': ',\n",
       " 'o,',\n",
       " 'nv',\n",
       " ':/',\n",
       " 'z.',\n",
       " '\"r',\n",
       " '.n',\n",
       " 'lw',\n",
       " '1b',\n",
       " 's™',\n",
       " 'ww',\n",
       " '3d',\n",
       " 'eo',\n",
       " '-d',\n",
       " 'gr',\n",
       " 'y ',\n",
       " 'ek',\n",
       " 'i ',\n",
       " 'ap',\n",
       " '3 ',\n",
       " '7.',\n",
       " \"'t\",\n",
       " 'bl',\n",
       " 'ié',\n",
       " '₹8',\n",
       " 'tg',\n",
       " 'gk',\n",
       " 'qu',\n",
       " 'bh',\n",
       " 'i’',\n",
       " '\" ',\n",
       " 'tf',\n",
       " 'n/',\n",
       " 'n2',\n",
       " '),',\n",
       " 'ah',\n",
       " '“g',\n",
       " 'x8',\n",
       " 'kü',\n",
       " '8-',\n",
       " 'sh',\n",
       " 'sq',\n",
       " 'u.',\n",
       " 'tw',\n",
       " 'rm',\n",
       " 'ea',\n",
       " 'fg',\n",
       " 'uk',\n",
       " 'c,',\n",
       " 'wn',\n",
       " 's)',\n",
       " '24',\n",
       " 'cx',\n",
       " '&e',\n",
       " '\".',\n",
       " 'e(',\n",
       " 'd’',\n",
       " 'dr',\n",
       " 'oy',\n",
       " 'ar',\n",
       " '14',\n",
       " '17',\n",
       " 's’',\n",
       " 'ce',\n",
       " 'xe',\n",
       " 'h)',\n",
       " 'oz',\n",
       " 'cs',\n",
       " 'g-',\n",
       " '(o',\n",
       " 'th',\n",
       " 'ax',\n",
       " 'a,',\n",
       " 'be',\n",
       " 'io',\n",
       " '妒是',\n",
       " '\"i',\n",
       " '0-',\n",
       " '/c',\n",
       " 'l’',\n",
       " 'p)',\n",
       " 'm…',\n",
       " 'ad',\n",
       " 'lt',\n",
       " ' u',\n",
       " 'ga',\n",
       " 'l.',\n",
       " '//',\n",
       " '5%',\n",
       " 'o\"',\n",
       " 'm\"',\n",
       " 'y.',\n",
       " 'ya',\n",
       " 'lr',\n",
       " '/s',\n",
       " 'jf',\n",
       " '(c',\n",
       " '#m',\n",
       " 'r1',\n",
       " '18',\n",
       " 'r)',\n",
       " 'i\"',\n",
       " 'nç',\n",
       " \"n'\",\n",
       " 'ul',\n",
       " \"' \",\n",
       " '(&',\n",
       " 'de',\n",
       " 'n\"',\n",
       " 'a.',\n",
       " 'mt',\n",
       " '4t',\n",
       " \"x'\",\n",
       " 'go',\n",
       " '\")',\n",
       " '45',\n",
       " '/d',\n",
       " '2g',\n",
       " 'rn',\n",
       " 'do',\n",
       " 'xf',\n",
       " 'ep',\n",
       " \"'r\",\n",
       " 'e’',\n",
       " 'uc',\n",
       " 'a ',\n",
       " '5:',\n",
       " '10',\n",
       " 'cy',\n",
       " '-3',\n",
       " 'r-',\n",
       " 'ua',\n",
       " 'p-',\n",
       " '88',\n",
       " 'hi',\n",
       " '-1',\n",
       " '86',\n",
       " '5.',\n",
       " '-a',\n",
       " 'ko',\n",
       " ' m',\n",
       " 'hp',\n",
       " '7t',\n",
       " 'm.',\n",
       " 'oe',\n",
       " '1 ',\n",
       " 'm?',\n",
       " ').',\n",
       " 'xl',\n",
       " 's/',\n",
       " '$9',\n",
       " 'vh',\n",
       " 'we',\n",
       " 'vf',\n",
       " '.p',\n",
       " ' k',\n",
       " 'oc',\n",
       " '-s',\n",
       " '”?',\n",
       " 'a\"',\n",
       " ' s',\n",
       " 'ig',\n",
       " 'nk',\n",
       " '1.',\n",
       " 'yd',\n",
       " 'up',\n",
       " 'ks',\n",
       " 'y:',\n",
       " 'cé',\n",
       " 'av',\n",
       " '93',\n",
       " 'uo',\n",
       " 'na',\n",
       " 'y-',\n",
       " '’,',\n",
       " '..',\n",
       " '5-',\n",
       " '\"t',\n",
       " 'ho',\n",
       " 'ue',\n",
       " '“w',\n",
       " 'o.',\n",
       " '.2',\n",
       " 'ln',\n",
       " 'dj',\n",
       " 'o’',\n",
       " '4,',\n",
       " '’?',\n",
       " 'f*',\n",
       " 'tn',\n",
       " '\"e',\n",
       " '0k',\n",
       " 'f ',\n",
       " 'zi',\n",
       " 'l,',\n",
       " '。\"',\n",
       " ' 3',\n",
       " 'cq',\n",
       " 'e]',\n",
       " 'ps',\n",
       " 'pp',\n",
       " 'al',\n",
       " 'k?',\n",
       " '16',\n",
       " 'sr',\n",
       " 'i-',\n",
       " 'h ',\n",
       " '’r',\n",
       " 'l ',\n",
       " 'g/',\n",
       " 'ph',\n",
       " 'fi',\n",
       " 'e”',\n",
       " 'n\\u200b',\n",
       " '5s',\n",
       " 'hw',\n",
       " '*k',\n",
       " \"'é\",\n",
       " 'ed',\n",
       " '-t',\n",
       " 'id',\n",
       " '\\xa0t',\n",
       " ' “',\n",
       " 'or',\n",
       " 'g\"',\n",
       " 'h-',\n",
       " 'ki',\n",
       " '92',\n",
       " '“f',\n",
       " 'fs',\n",
       " '(t',\n",
       " 'd ',\n",
       " 'ov',\n",
       " 'on',\n",
       " 'tj',\n",
       " 'fr',\n",
       " ' e',\n",
       " 'pl',\n",
       " 't?',\n",
       " 'lp',\n",
       " 'lk',\n",
       " 'tq',\n",
       " '(m',\n",
       " '8?',\n",
       " \"e'\",\n",
       " 'h\"',\n",
       " 'xp',\n",
       " '‘r',\n",
       " 'e\"',\n",
       " ') ',\n",
       " 'kl',\n",
       " 'h2',\n",
       " 'w2',\n",
       " 'bt',\n",
       " 'yi',\n",
       " \"y'\",\n",
       " '\"a',\n",
       " 'n-',\n",
       " '8,',\n",
       " '/l',\n",
       " 'w-',\n",
       " 'rt',\n",
       " '(b',\n",
       " '嫉妒',\n",
       " ' 8',\n",
       " 'yb',\n",
       " 'za',\n",
       " 't”',\n",
       " 'eq',\n",
       " '\"v',\n",
       " 'd\\xa0',\n",
       " 'uy',\n",
       " '.d',\n",
       " '.h',\n",
       " 'eb',\n",
       " 'ik',\n",
       " ' 5',\n",
       " 'ru',\n",
       " ' \"',\n",
       " 'bb',\n",
       " 'df',\n",
       " '/u',\n",
       " 'k,',\n",
       " 'z ',\n",
       " 'ts',\n",
       " '…e',\n",
       " 'cg',\n",
       " 'lf',\n",
       " 'sy',\n",
       " 'lo',\n",
       " 'ny',\n",
       " 't\\u200b',\n",
       " '73',\n",
       " 's-',\n",
       " 'bi',\n",
       " '/o',\n",
       " 'bg',\n",
       " 've',\n",
       " 'dw',\n",
       " 'il',\n",
       " 'ai',\n",
       " '[河',\n",
       " 'rc',\n",
       " '.9',\n",
       " '5/',\n",
       " 'tv',\n",
       " 's”',\n",
       " '76',\n",
       " 's.',\n",
       " 'wy',\n",
       " \"i'\",\n",
       " 'gh',\n",
       " '-w',\n",
       " 'aa',\n",
       " 'yl',\n",
       " 'bc',\n",
       " \"a'\",\n",
       " 'tc',\n",
       " 'gd',\n",
       " 'qe',\n",
       " 'ms',\n",
       " 'hn',\n",
       " 'l-',\n",
       " '50',\n",
       " \"'p\",\n",
       " \"'?\",\n",
       " 'cc',\n",
       " 'zo',\n",
       " 'x,',\n",
       " '/t',\n",
       " 'fl',\n",
       " 'wi',\n",
       " 'ol',\n",
       " 'qs',\n",
       " ' ‘',\n",
       " '.3',\n",
       " '\"d',\n",
       " 'd\"',\n",
       " '-5',\n",
       " ...}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_2_grams_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1204"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_2_grams_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for letter in all_2_grams_letters:\n",
    "    train[letter] = [sum([1 for l in question if l == letter]) for question in train[\"question_text\"]]\n",
    "    test[letter] = [sum([1 for l in question if l == letter]) for question in test[\"question_text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let see, if there is any predictive validity in this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_len_letter = LogisticRegression(solver=\"lbfgs\", max_iter=10000)\n",
    "log_len_letter = log_len_letter.fit(numpy.array(train[list(all_2_grams_letters)]).reshape(-1, len(all_2_grams_letters)), train[\"target\"])\n",
    "log_len_letter_prediction = log_len_letter.predict(numpy.array(test[list(all_2_grams_letters)]).reshape(-1, len(all_2_grams_letters)))\n",
    "log_len_letter_confusion_matrix = confusion_matrix(test[\"target\"], log_len_letter_prediction)\n",
    "log_len_letter_confusion_matrix\n",
    "(log_len_letter_confusion_matrix[0][0] + log_len_letter_confusion_matrix[1][1])/sum(sum(log_len_letter_confusion_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the answer seems to be no, so I am not going to analize it with higher level n-grams based on letters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"words\">Analysis based on Words</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"stopwords\">Number of stop words in a sentence</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I am going to see, if there are any stopwords in the sentence and count how many are these. These are usually functunal words, or very frequent ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.391111111111111, 6.647777777777778)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"stopwords\"] = [len([word for word in word_tokenize(question) if len(word) > 1 and word in stopWords]) for question in train[\"question_text\"]]\n",
    "test[\"stopwords\"] = [len([word for word in word_tokenize(question) if len(word) > 1 and word in stopWords]) for question in test[\"question_text\"]]\n",
    "numpy.mean(train.query('target == 0')[\"stopwords\"]), numpy.mean(train.query('target == 1')[\"stopwords\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, there seems to be some difference. Let us try now, how well does it do predicting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_len_letter = LogisticRegression(solver=\"lbfgs\", max_iter=10000)\n",
    "log_len_letter = log_len_letter.fit(numpy.array(train[\"stopwords\"]).reshape(-1, 1), train[\"target\"])\n",
    "log_len_letter_prediction = log_len_letter.predict(numpy.array(test[\"stopwords\"]).reshape(-1, 1))\n",
    "log_len_letter_confusion_matrix = confusion_matrix(test[\"target\"], log_len_letter_prediction)\n",
    "log_len_letter_confusion_matrix\n",
    "(log_len_letter_confusion_matrix[0][0] + log_len_letter_confusion_matrix[1][1])/sum(sum(log_len_letter_confusion_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it seems to have some predictive validity. It can predict it to the 62%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"phonetics\">Phonetics</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More for fun than anything else, I am also going to try and include some phonetics. It is supposed to be an important part of lingustics, but I don't really know much about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the phonetics of a sentence are then shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ˈɑːlɪvɚ kwˈiːn ɪz ðə mˈeɪɚ ðætðə stˈɑːɹlɪŋ sˈɪɾi dɪzˈɜːvz\\n'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_output([\"espeak\", \"-q\", \"--ipa\", '-v', 'en-us', \"Oliver Queen is the mayor that the Starling City deserves\"]).decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ː prepresents the long sound, so I can calculate, how many long sounds are in the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"longsound\"] = [len([letter for letter in check_output([\"espeak\", \"-q\", \"--ipa\", '-v', 'en-us', question]).decode('utf-8') if letter == \"ː\"]) for question in train[\"question_text\"]]\n",
    "test[\"longsound\"] = [len([letter for letter in check_output([\"espeak\", \"-q\", \"--ipa\", '-v', 'en-us', question]).decode('utf-8') if letter == \"ː\"])for question in test[\"question_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.5522222222222224, 5.03)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.mean(train.query('target == 0')[\"longsound\"]), numpy.mean(train.query('target == 1')[\"longsound\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And surprisingly, there is a difference. So lets see how well can this predict the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_len_letter = LogisticRegression(solver=\"lbfgs\", max_iter=10000)\n",
    "log_len_letter = log_len_letter.fit(numpy.array(train[\"longsound\"]).reshape(-1, 1), train[\"target\"])\n",
    "log_len_letter_prediction = log_len_letter.predict(numpy.array(test[\"longsound\"]).reshape(-1, 1))\n",
    "log_len_letter_confusion_matrix = confusion_matrix(test[\"target\"], log_len_letter_prediction)\n",
    "log_len_letter_confusion_matrix\n",
    "(log_len_letter_confusion_matrix[0][0] + log_len_letter_confusion_matrix[1][1])/sum(sum(log_len_letter_confusion_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, the difference is relativly small. That is too bad. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can also check the number of sounds in the sentence (which could be different from the number of letters, when we are dealing with the langauge like English)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"phonlength\"] = [len([letter for letter in check_output([\"espeak\", \"-q\", \"--ipa\", '-v', 'en-us', question]).decode('utf-8')]) for question in train[\"question_text\"]]\n",
    "test[\"phonlength\"] = [len([letter for letter in check_output([\"espeak\", \"-q\", \"--ipa\", '-v', 'en-us', question]).decode('utf-8')])for question in test[\"question_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78.73777777777778, 110.77888888888889)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.mean(train.query('target == 0')[\"phonlength\"]), numpy.mean(train.query('target == 1')[\"phonlength\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with letters, there is a difference. But how big is it really?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_len_letter = LogisticRegression(solver=\"lbfgs\", max_iter=10000)\n",
    "log_len_letter = log_len_letter.fit(numpy.array(train[\"phonlength\"]).reshape(-1, 1), train[\"target\"])\n",
    "log_len_letter_prediction = log_len_letter.predict(numpy.array(test[\"phonlength\"]).reshape(-1, 1))\n",
    "log_len_letter_confusion_matrix = confusion_matrix(test[\"target\"], log_len_letter_prediction)\n",
    "log_len_letter_confusion_matrix\n",
    "(log_len_letter_confusion_matrix[0][0] + log_len_letter_confusion_matrix[1][1])/sum(sum(log_len_letter_confusion_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, better than the long sounds, but it actually faired a bit less well, than the length based just on letters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"typeofsounds\">Type of sounds</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of sounds also have a different effect on the psychology of people. This can allow them to produce different effects. So maybe people that want to be taken seriouslly use different phonology than people staring the flame wars. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the citation, that was in my notes: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*\"Among the discoveries Leben made: Fricatives convey “faster” and “smaller” — as do vowels that are voiced near the front of the mouth, like the a in “bat” or the i in “hid.” Plosives, or stops, convey “slower” and “bigger” — as do vowels that are voiced at the back of the throat, like the o in “token” or the double o’s in “food.” So-called voiceless stops like k, p, and t are more alive and daring than voiced stops like b, d and g, while the voiceless convey less luxury than the voiced. And all sound-symbolic effects manifest differently depending on context. They take on properties of the product being named.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the IPA chart from: http://www.internationalphoneticalphabet.org/ipa-sounds/ipa-chart-with-sounds/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "phonology_front = \"iyɪʏeøɛæ\"\n",
    "phonology_back = \"ɨʉʊɘɵəœɜɞɐaä\"\n",
    "phonology_plosive = \"pbtd\"\n",
    "phonology_frictive = \"ɸβfvθðszʃʒ\"\n",
    "phonology_nasal = \"mɱn\"\n",
    "phonology_lat_frictive = \"ɬɮ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phonological_groups(text):\n",
    "    fast_list = phonology_front + phonology_frictive + phonology_lat_frictive\n",
    "    slow_list = phonology_back + phonology_nasal + phonology_plosive\n",
    "    i = j = 0\n",
    "    for fast in fast_list:\n",
    "        i += len([1 for letter in text if fast])\n",
    "    for slow in slow_list:\n",
    "        j += len([1 for letter in text if fast])\n",
    "    return i, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"phon_fast\"] = [get_phonological_groups(question)[0] for question in train[\"question_text\"]]\n",
    "test[\"phon_fast\"] = [get_phonological_groups(question)[0] for question in test[\"question_text\"]]\n",
    "train[\"phon_slow\"] = [get_phonological_groups(question)[1] for question in train[\"question_text\"]]\n",
    "test[\"phon_slow\"] = [get_phonological_groups(question)[1] for question in test[\"question_text\"]]\n",
    "train[\"phon_speed_ratio\"] = train[\"phon_fast\"]/train[\"phon_slow\"]\n",
    "test[\"phon_speed_ratio\"] = test[\"phon_fast\"]/test[\"phon_slow\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see, if there are some differences based on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1338.2444444444445, 1952.4666666666667)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.mean(train.query('target == 0')[\"phon_fast\"]), numpy.mean(train.query('target == 1')[\"phon_fast\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1271.3322222222223, 1854.8433333333332)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.mean(train.query('target == 0')[\"phon_slow\"]), numpy.mean(train.query('target == 1')[\"phon_slow\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0526315789473684, 1.0526315789473684)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.mean(train.query('target == 0')[\"phon_speed_ratio\"]), numpy.mean(train.query('target == 1')[\"phon_speed_ratio\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the ratio is basically the same, the difference is probablly just the result of different lengths. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"morphology\">Morphology</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"morphologypinker\">Morphology - Pinker Example</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the book The Language Instinkt, Pinker mentiones how English is morphologically rich, when it comes to creating words from different types of words. For example, fish can be used as noun (the fish) but also as a verb (to fish). He also produces the number of word endings, that English is using to do this. This list is what I am going to use here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_add_ons = [\"able\", \"age\", \"al\", \"an\", \"ant\", \"ance\", \"ary\", \"ate\", \"ed\", \"en\", \"er\", \"ful\", \"hood\", \n",
    "                \"ic\", \"ify\", \"ion\", \"ish\", \"ism\", \"iest\", \"ity\", \"ive\", \"ize\", \"ly\", \"ment\", \"ness\", \"ory\",\n",
    "               \"ous\", \"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_addons(words_add_ons, text):\n",
    "    all_examples = 0\n",
    "    for add_on in words_add_ons:\n",
    "        all_examples += len(re.findall(add_on, text))\n",
    "    return all_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"morphologypinker\"] = [find_all_addons(word_add_ons, question) for question in train[\"question_text\"]]\n",
    "test[\"morphologypinker\"] = [find_all_addons(word_add_ons, question) for question in test[\"question_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.11, 8.027777777777779)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.mean(train.query('target == 0')[\"morphologypinker\"]), numpy.mean(train.query('target == 1')[\"morphologypinker\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it seems that there is some difference again. But let me check, how predictive it is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_len_letter = LogisticRegression(solver=\"lbfgs\", max_iter=10000)\n",
    "log_len_letter = log_len_letter.fit(numpy.array(train[\"morphologypinker\"]).reshape(-1, 1), train[\"target\"])\n",
    "log_len_letter_prediction = log_len_letter.predict(numpy.array(test[\"morphologypinker\"]).reshape(-1, 1))\n",
    "log_len_letter_confusion_matrix = confusion_matrix(test[\"target\"], log_len_letter_prediction)\n",
    "log_len_letter_confusion_matrix\n",
    "(log_len_letter_confusion_matrix[0][0] + log_len_letter_confusion_matrix[1][1])/sum(sum(log_len_letter_confusion_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, I think that by now I can say that most things are predictive in about this rang. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"pos\">Parts of Speech</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me now try the parts of speech tagging. I am hoping that this will allow me a clean way of tagging nouns, since I have a idea or two, that I want to try with it later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags = set()\n",
    "_ = [[all_tags.add(tag) for word, tag in pos_tag(word_tokenize(question), tagset=\"universal\")] for question in train[\"question_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " 'ADJ',\n",
       " 'ADP',\n",
       " 'ADV',\n",
       " 'CONJ',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'NUM',\n",
       " 'PRON',\n",
       " 'PRT',\n",
       " 'VERB',\n",
       " 'X'}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "for current_tag in all_tags:\n",
    "    train[\"tag_\" + current_tag] = [len([tag for word, tag in pos_tag(word_tokenize(question), tagset=\"universal\") if tag==current_tag])for question in train[\"question_text\"]]\n",
    "    test[\"tag_\" + current_tag] = [len([tag for word, tag in pos_tag(word_tokenize(question), tagset=\"universal\") if tag==current_tag])for question in test[\"question_text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now see, if there are any big differences in the three most prominent groups: nouns, adjectives and verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.482222222222222, 5.007777777777778)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.mean(train.query('target == 0')[\"tag_NOUN\"]), numpy.mean(train.query('target == 1')[\"tag_NOUN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9988888888888889, 1.4955555555555555)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.mean(train.query('target == 0')[\"tag_ADJ\"]), numpy.mean(train.query('target == 1')[\"tag_ADJ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.5533333333333332, 3.758888888888889)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.mean(train.query('target == 0')[\"tag_VERB\"]), numpy.mean(train.query('target == 1')[\"tag_VERB\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be some minimal differences. Let me now also check with the logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_names = [\"tag_\" + tag for tag in all_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.675"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_len_letter = LogisticRegression(solver=\"lbfgs\", max_iter=10000)\n",
    "log_len_letter = log_len_letter.fit(numpy.array(train[tag_names]).reshape(-1, len(tag_names)), train[\"target\"])\n",
    "log_len_letter_prediction = log_len_letter.predict(numpy.array(test[tag_names]).reshape(-1, len(tag_names)))\n",
    "log_len_letter_confusion_matrix = confusion_matrix(test[\"target\"], log_len_letter_prediction)\n",
    "log_len_letter_confusion_matrix\n",
    "(log_len_letter_confusion_matrix[0][0] + log_len_letter_confusion_matrix[1][1])/sum(sum(log_len_letter_confusion_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, there is at least some predictive validity in this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"diversity\">Lexical Diversity</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I was reading the articles about the fanfiction, I have came accross the article that talked about lexical diversity. So I used the code from here: https://github.com/jfrens/lexical_diversity/, to calculate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input word list should be at least 50 in length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-c6bbbcc57386>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lexical_diversity\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlexical_diversity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmtld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lexical_diversity\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlexical_diversity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmtld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-107-c6bbbcc57386>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lexical_diversity\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlexical_diversity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmtld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lexical_diversity\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlexical_diversity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmtld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/Files/AnalysisProject-Kaggle/lexical_diversity.py\u001b[0m in \u001b[0;36mmtld\u001b[0;34m(word_array, ttr_threshold)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input should be a list of strings, rather than a string. Try using string.split()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_array\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input word list should be at least 50 in length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmtld_calc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mttr_threshold\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmtld_calc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mttr_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input word list should be at least 50 in length"
     ]
    }
   ],
   "source": [
    "train[\"lexical_diversity\"] = [lexical_diversity.mtld(question.split()) for question in train[\"question_text\"]]\n",
    "test[\"lexical_diversity\"] = [lexical_diversity.mtld(question.split()) for question in test[\"question_text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, because of the length, this did not work at all. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"bayes\">Bayes Analysis of all Features</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the end, I will try to see, how good is the prediction, if all features are used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_features = train.shape[1] - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[62, 38],\n",
       "       [23, 77]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bayes = MultinomialNB()\n",
    "model_bayes = model_bayes.fit(numpy.array(train.iloc[:, 3:]).reshape(-1, number_of_features), train[\"target\"])\n",
    "model_bayes_prediction = model_bayes.predict(numpy.array(test.iloc[:, 3:]).reshape(-1, number_of_features))\n",
    "model_bayes_confusion_matrix = confusion_matrix(test[\"target\"], model_bayes_prediction)\n",
    "model_bayes_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.695"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model_bayes_confusion_matrix[0][0] + model_bayes_confusion_matrix[1][1])/sum(sum(model_bayes_confusion_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"logistic\">Logistic Regression of all Features</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[71, 29],\n",
       "       [23, 77]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_logistic = LogisticRegression(solver=\"lbfgs\", max_iter=10000)\n",
    "model_logistic = log_len_letter.fit(numpy.array(train.iloc[:, 3:]).reshape(-1, number_of_features), train[\"target\"])\n",
    "model_logistic_prediction = log_len_letter.predict(numpy.array(test.iloc[:, 3:]).reshape(-1, number_of_features))\n",
    "model_logistic_confusion_matrix = confusion_matrix(test[\"target\"], model_logistic_prediction)\n",
    "model_logistic_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model_logistic_confusion_matrix[0][0] + model_logistic_confusion_matrix[1][1])/sum(sum(model_logistic_confusion_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
